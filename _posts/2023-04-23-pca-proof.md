---
layout: post
title: "Proof of PCA (ongoing)"
author: "wano"
excerpt_separator: <!--more-->
tags: ['math']
use_math: true
lastmode: 2023-04-23 13:00:00
sitemap:
  changefreq: weekly
  priority: 0.5
---

Why eigenvectors of a covariance matrix become principal component axes<!--more-->

*본 포스트에는 수식이 포함되어 있습니다. 분수 등의 수식이 정상적으로 보이지 않는 경우에는 수식을 마우스 오른쪽 버튼으로 클릭한 후 "Math Renderer"를 SVG로 바꿔주세요.*


$n$개의 **특성(feature)**을 가지는 확률변수 $X$에 대한 $n$개의 **표본(sample)** $p_i$가 주어졌다고 가정해보겠습니다.

중심점 이동(centering), 스케일링(scaling) 등의 전처리 과정(pre-processing)을 거친 데이터는 다음과 같은 $m \times n$ 행렬 $\mathbf{D}$로 나타낼 수 있습니다.

$$
\mathbf{D}

\; = \;

\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1n} \\\
x_{11} & x_{22} & \cdots & x_{2n} \\\
\vdots & \vdots & & \vdots \\\
x_{m1} & x_{m2} & \cdots & x_{mn}
\end{bmatrix}

\; = \;

\begin{bmatrix}
- \mathbf{p}_1 - \\\
- \mathbf{p}_2 - \\\
\vdots \\
- \mathbf{p}_m - \\\
\end{bmatrix}

\; = \;

\begin{bmatrix}
| & | & \cdots & | \\\
\mathbf{f}_1 & \mathbf{f}_2 & \cdots & \mathbf{f}_n \\\
| & | & \cdots & |
\end{bmatrix}
$$


여기서 $\mathbf{p}_i$와 $\mathbf{f}_j$는 각각 행렬 $\mathbf{D}$의 $i$번째 행과 $j$번째 열을 의미합니다. 즉, $\mathbf{p}_i$는 $i$번째 **표본(sample)**이고, $\mathbf{f}_j$는 $j$번째 **특성(feature)**을 나타냅니다. 따라서 $m$은 표본의 개수이고, $n$은 특성의 개수를 의미합니다.

이때 $a$번째 특성과 $b$번째 특성의 **상관관계(correlation)**는 어떻게 정량화할 수 있을까요? $a$번째 특성을 나타내는 벡터는$\mathbf{f}_a$이고, $b$번째 특성을 나타내는 벡터는 $\mathbf{f}_b$이기 때문에 두 벡터를 내적하면 두 가지 특성 사이의 상관관계를 정량화하여 나타낼 수 있습니다. 왜냐하면 두 벡터의 방향이 같아서 두 벡터가 이루는 각의 크기가 0일 때 최대값을 출력하고, 두 벡터가 서로 수직어어서 둘 사이의 상관관계가 전혀 없을 때 0을 출력하는 용도의 연산으로 사용하는 것이 바로 내적이기 때문입니다. 다만 표본의 개수에 따라서 결과의 범위가 너무 많이 달라지는 것을 피하기 위해서 두 벡터의 내적을 $m$으로 나누어서 정규화시킨 값을 두 특성 사이의 상관관계 계측값으로 사용합니다.

