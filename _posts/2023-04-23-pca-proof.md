---
layout: post
title: "Proof of PCA (ongoing)"
author: "wano"
excerpt_separator: <!--more-->
tags: ['math']
use_math: true
lastmode: 2023-04-23 13:00:00
sitemap:
  changefreq: weekly
  priority: 0.5
---

Why eigenvectors of a covariance matrix become principal component axes<!--more-->

*본 포스트에는 수식이 포함되어 있습니다. 분수 등의 수식이 정상적으로 보이지 않는 경우에는 수식을 마우스 오른쪽 버튼으로 클릭한 후 "Math Renderer"를 SVG로 바꿔주세요.*

$n$개의 **특성(feature)**을 가지는 확률변수 $X$에 대한 $n$개의 **표본(sample)** $p_i$가 주어졌다고 가정해보겠습니다.

중심점 이동(centering), 스케일링(scaling) 등의 전처리 과정(pre-processing)을 거친 데이터는 다음과 같은 $m \times n$ 행렬 $\mathbf{D}$로 나타낼 수 있습니다.

<center><img src="https://cgvfxmath.github.io/assets/img/pca_proof_01.png" width="100%"></center>

여기서 $\mathbf{p}_i$와 $\mathbf{f}_j$는 각각 행렬 $\mathbf{D}$의 $i$번째 행과 $j$번째 열을 의미합니다. 즉, $\mathbf{p}_i$는 $i$번째 **표본(sample)**이고, $\mathbf{f}_j$는 $j$번째 **특성(feature)**을 나타냅니다. 따라서 $m$은 표본의 개수이고, $n$은 특성의 개수를 의미합니다.

이때 $a$번째 특성과 $b$번째 특성의 **상관관계(correlation)**는 어떻게 정량화할 수 있을까요? $a$번째 특성을 나타내는 벡터는 $\mathbf{f}_a$이고, $b$번째 특성을 나타내는 벡터는 $\mathbf{f}_b$이기 때문에 두 벡터를 내적하면 두 가지 특성 사이의 상관관계를 정량화하여 나타낼 수 있습니다. 왜냐하면 내적이란 연산은 두 벡터의 방향이 일치하여 두 벡터가 이루는 각이 0일 때 최대값을 반환하며, 반대로 두 벡터가 수직하여 상관관계가 전혀 없을 때는 0을 반환하는 연산이기 때문입니다. 다만 표본의 개수에 따라서 결과의 범위가 너무 많이 달라지는 것을 피하기 위해서 두 벡터의 내적을 $m$으로 나누어서 정규화시킨 값을 두 특성 사이의 상관관계 계측값으로 사용합니다.

<p style="text-align: center;">$c_{ab} = \frac{1}{m} \mathbf{f}_a^\text{T} \mathbf{f}_b$</p>

이러한 상관관계 계측값을 $(a,b)$ 성분으로 가지는 $m \times n$ 행렬을 확률변수 $\mathbf{X}$의 **공분산 행렬(covariance matrix)**이라고 합니다.

<center><img src="https://cgvfxmath.github.io/assets/img/pca_proof_02.png" width="100%"></center>

즉, 공분산 행렬 $\mathbf{C}(\mathbf{X})$의 $(a,b)$ 성분은 특성변수 $\mathbf{f}_a$와 $\mathbf{f}_b$ 사이의 상관관계 정보를 알려줍니다. 그리고 이러한 공분산 행렬은 다음과 같은 성질을 가집니다.

확률변수 $\mathbf{X}$의 공분산 행렬이 $\mathbf{C}$일 때
1. 확률변수 $\mathbf{X}$의 $\mathbf{u}$ 방향으로의 분산은 $\mathbf{u}^\text{T} \mathbf{C} \mathbf{u}$ 입니다. (단, $\mathbf{u}^\text{T} \mathbf{u} = 1$)
2. 공분산 행렬 $\mathbf{C}$의 $i$번째 고유값인 $\lambda_i$는 $i$번째 고유벡터 $\mathbf{v}_i$ 방향으로의 분산을 의미합니다.
3. 큰 분산값을 가지는 방향은 차례대로 $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3, \cdots$ 입니다. (단, $\lambda_1 > \lambda_2 > \lambda_3 > \cdots$)
   즉, 가장 큰 분산을 가지는 방향은 가장 큰 고유값을 가지는 고유벡터의 방향입니다.

