---
layout: post
title: "Which GPU for Deep Learning"
author: "wano"
excerpt_separator: <!--more-->
tags: ['dev', 'gpu', 'deep learning']
use_math: true
lastmode: 2023-03-28 09:00:00
sitemap:
  changefreq: weekly
  priority: 0.5
---

Graphics Card Buying Guide for Deep Learning<!--more-->

이 포스트는 다음 두 글의 내용을 러프하게 요약한 글입니다.

[Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning)

[딥러닝 서버 구매/구축 가이드](https://www.2cpu.co.kr/deep/982?&sst=wr_good&sod=desc&sop=and&page=1)

---

대부분의 수치해석 문제가 그러하듯이 딥러닝 또한 많은 수의 행렬 곱셈(matrix multiplication) 과정을 거치게 된다. 따라서 그래픽 카드 사양중에서 가장 중요한 것은 텐서 코어(Tensor Core)의 개수이다.

그 다음으로 중요한 것은 메모리 대역폭(bandwidth)이다. 전역 메모리에서 데이터를 가져와서 연산을 수행하게 되는데, 대역폭이 작아서 필요한 데이터가 코어에 적시에 전달되지 않으면 연산을 수행하는 코어가 유휴상태가 된다. 때에 따라서는 30%의 코어가 쉬는 경우도 발생할 수 있다. A100의 메모리 대역폭은 1,555 GB/s이고, V100은 900 GB/s이다. 따라서 메모리 대역폭의 관점에서만 보면 A100이 V100 보다 1555/900=1.73배 더 빠르다고 볼 수 있다.

그 다음 중요한 것은 로컬 공유 메모리의 크기이다. 캐시 메모리라고도 하는데 캐시 메모리 용량이 클수록 속도가 느린 공유 메모리에 접근하는 빈도를 줄일 수 있다. Volta, Turing, Ampere 아키텍쳐별 공유 메모리의 크기는 각각 96 kb, 64 kb, 164 kb이다. 이로 인해 Ampere 아키텍쳐는 Volta 아키텍쳐에 비해 약 2~5% 정도의 성능 향상이 있다.

작업에 필요한 충분한 크기의 메모리가 있는지도 확인해야 한다. Transformer의 경우 최소 11 GB의 메모리가 필요하고, 보통 24 GB 이상의 메모리가 필요하다.

종합하면 Tesla A100은 V100에 비해 NLP(자연어 처리)의 경우 약 1.7배, 컴퓨터 비전(CNN)의 경우 1.45배 빠르다.

RTX 2080 Ti와 비교하여 RTX 3090은 1.5배 정도의 속도가 향상되었지만 가격은 15% 정도만 인상되었다. 즉, Ampere RTX 30은 Turing RTX 20 시리즈에 비해 가격대비 성능이 향상되었다. 따라서, 일반적인 기준으로 봤을 때 현시점에서는 RTX 3090을 구매하는 것이 가장 좋은 선택이다. 성능 경쟁을 하는 것이 아니라면 RTX 3080를 선택하는 것도 나쁘지 않다. V100이나 A100은 가격대비 성능비가 나쁘기 때문에 (너무 비쌈) 반드시 사야하는 이유가 있는 것이 아니라면 추천하지 않는다. GTX 16 시리즈는 텐서 코어가 없기 때문에 절대 비추.

이미 RTX 2080 Ti 이상의 그래픽 카드를 가지고 있다면 RTX 3090으로 업그레이드 하는 것이 큰 의미가 없을 수 있다. RTX 2080 Ti는 꽤 훌륭하며, RTX 30 시리즈의 냉각 및 전력 공급 문제를 해결하기 위해서 들이는 노력 대비 성능 향상은 미미할 수 있기 때문이다.

RTX 3080/3090 Ti가 출시될런지는 현재로서 알 수 없다. 가격과 성능 등을 고려했을 때 RTX 3080 Ti를 대체하는 것이 RTX 3090으로 보인다. 하지만 이것은 추측일 뿐이므로 RTX 3090 Ti를 기대한다면 한 두 달 정도 관련 뉴스들을 꼼꼼히 살펴보는 것이 좋겠다.

여러 개의 그래픽 카드들을 병렬화하여 사용하면 성능 향상이 있기는 하지만 오버헤드로 인해 성능이 선형적으로 증가하지는 않는다. A100 시스템(NVLink 3.0)은 V100 시스템(NVLink 2.0)에 비해 더 나은 속도를 제공한다. 8대 연결 사용시 A100은 7배, V100은 6.67배의 속도가 향상된다.

여러 대의 그래픽 카드를 병렬로 연결하여 사용할 경우에는 냉각 및 전력 솔루션을 잘 설계해야 한다. RTX 30시리즈는 새로운 팬(fan) 디자인을 가지고 있다. 하지만, 나란히 쌓는 형태로 설치했을 때 어떤 문제가 발생할지 아직 불분명하다. 1~2대를 이렇게 사용할 때에는 큰 문제가 없을 가능성이 크지만, 3~4대를 이런식으로 사용하게 되면 문제가 발생할 가능성이 있다. 4대의 RTX 3090을 위해 4 × 350 = 1,400 W의 전원을 공급하는 것도 간단한 문제는 아니다. 1,600 W 용량의 전원 공급 장치 (PSU)는 쉽게 구할 수 있지만 CPU와 메인 보드에 전력을 공급하는데 200 W는 너무 빡빡할 수 있기 때문이다. 냉각이 잘 안되는 고사양의 그래픽 카드 구성은 냉각이 잘 되는 저사양의 그래픽 카드 구성보다 성능이 떨어질 수도 있다.

